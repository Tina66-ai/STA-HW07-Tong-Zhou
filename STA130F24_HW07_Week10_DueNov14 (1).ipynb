{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d66d4cf7",
   "metadata": {},
   "source": [
    "# STA130 Homework 07\n",
    "\n",
    "Please see the course [wiki-textbook](https://github.com/pointOfive/stat130chat130/wiki) for the list of topics covered in this homework assignment, and a list of topics that might appear during ChatBot conversations which are \"out of scope\" for the purposes of this homework assignment (and hence can be safely ignored if encountered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2c467d",
   "metadata": {},
   "source": [
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Introduction</u></summary>\n",
    "\n",
    "### Introduction\n",
    "    \n",
    "A reasonable characterization of STA130 Homework is that it simply defines a weekly reading comprehension assignment. \n",
    "Indeed, STA130 Homework essentially boils down to completing various understanding confirmation exercises oriented around coding and writing tasks.\n",
    "However, rather than reading a textbook, STA130 Homework is based on ChatBots so students can interactively follow up to clarify questions or confusion that they may still have regarding learning objective assignments.\n",
    "\n",
    "> Communication is a fundamental skill underlying statistics and data science, so STA130 Homework based on ChatBots helps practice effective two-way communication as part of a \"realistic\" dialogue activity supporting underlying conceptual understanding building. \n",
    "\n",
    "It will likely become increasingly tempting to rely on ChatBots to \"do the work for you\". But when you find yourself frustrated with a ChatBots inability to give you the results you're looking for, this is a \"hint\" that you've become overreliant on the ChatBots. Your objective should not be to have ChatBots \"do the work for you\", but to use ChatBots to help you build your understanding so you can efficiently leverage ChatBots (and other resources) to help you work more efficiently.<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Instructions</u></summary>\n",
    "\n",
    "### Instructions\n",
    "    \n",
    "1. Code and write all your answers (for both the \"Pre-lecture\" and \"Post-lecture\" HW) in a python notebook (in code and markdown cells) \n",
    "    \n",
    "> It is *suggested but not mandatory* that you complete the \"Pre-lecture\" HW prior to the Monday LEC since (a) all HW is due at the same time; but, (b) completing some of the HW early will mean better readiness for LEC and less of a \"procrastentation cruch\" towards the end of the week...\n",
    "    \n",
    "2. Paste summaries of your ChatBot sessions (including link(s) to chat log histories if you're using ChatGPT) within your notebook\n",
    "    \n",
    "> Create summaries of your ChatBot sessions by using concluding prompts such as \"Please provide a summary of our exchanges here so I can submit them as a record of our interactions as part of a homework assignment\" or, \"Please provide me with the final working verson of the code that we created together\"\n",
    "    \n",
    "3. Save your python jupyter notebook in your own account and \"repo\" on [github.com](github.com) and submit a link to that notebook though Quercus for assignment marking<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Prompt Engineering?</u></summary>\n",
    "    \n",
    "### Prompt Engineering? \n",
    "    \n",
    "The questions (as copy-pasted prompts) are designed to initialize appropriate ChatBot conversations which can be explored in the manner of an interactive and dynamic textbook; but, it is nonetheless **strongly recommendated** that your rephrase the questions in a way that you find natural to ensure a clear understanding of the question. Given sensible prompts the represent a question well, the two primary challenges observed to arise from ChatBots are \n",
    "\n",
    "1. conversations going beyond the intended scope of the material addressed by the question; and, \n",
    "2. unrecoverable confusion as a result of sequential layers logial inquiry that cannot be resolved. \n",
    "\n",
    "In the case of the former (1), adding constraints specifying the limits of considerations of interest tends to be helpful; whereas, the latter (2) is often the result of initial prompting that leads to poor developments in navigating the material, which are likely just best resolve by a \"hard reset\" with a new initial approach to prompting.  Indeed, this is exactly the behavior [hardcoded into copilot](https://answers.microsoft.com/en-us/bing/forum/all/is-this-even-normal/0b6dcab3-7d6c-4373-8efe-d74158af3c00)...\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f84f1d",
   "metadata": {},
   "source": [
    "\n",
    "### Marking Rubric (which may award partial credit) \n",
    "\n",
    "- [0.1 points]: All relevant ChatBot summaries [including link(s) to chat log histories if you're using ChatGPT] are reported within the notebook\n",
    "- [0.3 points]: Well-communicated, clear demonstration of the \"model building\" process and techniques of \"Question 4\"\n",
    "- [0.3 points]: Well-communicated, clear demonstration of the \"model building\" process and techniques of \"Question 7\"\n",
    "- [0.3 points]: Well-communicated, clear demonstration of the \"model building\" process and techniques of \"Question 9\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7ef03a",
   "metadata": {},
   "source": [
    "## \"Pre-lecture\" HW [*completion prior to next LEC is suggested but not mandatory*]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf63d8b",
   "metadata": {},
   "source": [
    "### 1. Explain succinctly in your own words (but working with a ChatBot if needed)...<br>\n",
    "\n",
    "1. the difference between **Simple Linear Regression** and **Multiple Linear Regression**; and the benefit the latter provides over the former\n",
    "\n",
    "\n",
    "2. the difference between using a **continuous variable** and an **indicator variable** in **Simple Linear Regression**; and these two **linear forms**\n",
    "\n",
    "\n",
    "3. the change that happens in the behavior of the model (i.e., the expected nature of the data it models) when a single **indicator variable** is introduced alongside a **continuous variable** to create a **Multiple Linear Regression**; and these two **linear forms** (i.e., the **Simple Linear Regression** versus the **Multiple Linear Regression**)\n",
    "\n",
    "\n",
    "4. the effect of adding an **interaction** between a **continuous** and an **indicator variable** in **Multiple Linear Regression** models; and this **linear form**\n",
    "\n",
    "\n",
    "5. the behavior of a **Multiple Linear Regression** model (i.e., the expected nature of the data it models) based only on **indicator variables** derived from a **non-binary categorical variable**; this **linear form**; and the necessarily resulting **binary variable encodings** it utilizes\n",
    "       \n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _The requested **linear forms** are **equations**, and answers must include **equations** along with the explanations that interpret the **linear forms**. Write you **linear forms** using a style something like_\n",
    "> \n",
    "> - _\"outcome = $\\beta_0$ + $\\beta_A$ predictorA + $\\beta_B$ 1(predictorB)\"_ \n",
    "> - _where the \"1(.)\" notation is for indicator variables_ \n",
    "> - _or feel free to use an similar alternative if a ChatBot provides you with another notation you think is clearer and like better if you prefer_\n",
    ">\n",
    "> _DO INCLUDE the **intercept** in your **linear forms**. You don't have to include notation related to the **error** term since this is essentially always assumed (and, actually, we usually don't even bother to include the **intercept** in such shorthand specifications either, for the same reason), but don't forget to include the **intercept** here this time (for practice). The modeling **assumptions** do not need to be addressed beyond this, but explanations will likely address the number of variables and the essential use-case (perhaps illustrated through examples) the different models imply._    \n",
    "> \n",
    "> _Answers to the final question above should address the notion of a \"baseline\" group and it's role for **model interpretation**, why \"number of categories minus one\" **indicator variables** are used to represent the original **categorical variable**, and the relationship between the **binary** and **categorical variables** that are relevant for this model specification. An example use-case would likely be helpful for illustration here._ \n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38a8154",
   "metadata": {},
   "source": [
    "Difference Between Simple Linear Regression (SLR) and Multiple Linear Regression (MLR); and the Benefits of MLR Over SLR\n",
    "Simple Linear Regression (SLR) involves modeling the relationship between a dependent variable (Y) and a single independent variable (X) as a straight line. The equation is:\n",
    "ùëå=ùõΩ0+ùõΩ1ùëã+ùúñ where Y is the dependent variable, ùëã is the independent variable, Œ≤0‚Äãis the intercept, ùõΩ1 is the slope, and œµ is the error term. SLR is used when there's a linear relationship between Y and one factor.\n",
    "Multiple Linear Regression (MLR) expands SLR by considering multiple independent variables. The equation becomes:Y=Œ≤0‚Äã+Œ≤1‚ÄãX1‚Äã+Œ≤2‚ÄãX 2‚Äã+‚ãØ+Œ≤ n‚ÄãX n‚Äã+œµwhere ùëã1ÔºåX2,...,ùëãn are multiple independent variables, and each Œ≤ represents the impact of each corresponding variable on Y. MLR is used when the outcome depends on several factors.\n",
    "Benefits of MLR over SLR:\n",
    "\n",
    "Better Fit: MLR can capture more complex relationships by considering multiple variables, leading to a more accurate and robust model than SLR.\n",
    "Control for Confounders: MLR helps control for additional factors that may influence Y, leading to a more accurate estimate of each variable's effect.Increased Explanatory Power: Including more variables increases the R value, indicating more of the variation in Y is explained by the model. Realistic Modeling: MLR better reflects real-world situations where multiple factors influence the outcome.\n",
    "Difference Between Using a Continuous Variable and an Indicator Variable in Simple Linear Regression\n",
    "Continuous Variable: A continuous variable can take any numeric value within a range, such as age, income, or height. In SLR, the model with a continuous variable is:Y==Œ≤0+Œ≤1X+œµ where X is the continuous independent variable. The coefficient Œ≤1 represents the average change in Y for each one-unit increase in X. For example, in a model where X is age and Y is income, Œ≤1 would indicate how much income increases with each additional year of age.\n",
    "Indicator (Dummy) Variable: An indicator variable is a categorical variable coded as 0 or 1, representing the presence or absence of a certain condition. In SLR, the model with an indicator variable is:Y=Œ≤0‚Äã+Œ≤1‚ÄãD+œµ where D is the indicator variable (0 or 1). The coefficient ùõΩ1 represents the difference in the mean value of Y between the two categories (e.g., male vs female, urban vs rural).Key Differences:Continuous Variable: Models gradual, proportional changes in Y for every unit change in ùëã.Indicator Variable: Models a discrete shift in Y between two groups, showing the difference between them.Changes in Model Behavior When Introducing an Indicator Variable Alongside a Continuous Variable to Create Multiple Linear Regression\n",
    "When you add an indicator variable (dummy variable) to a regression alongside a continuous variable, the model becomes a Multiple Linear Regression (MLR). The equation for this becomes:Y=Œ≤0‚Äã+Œ≤1‚ÄãX+Œ≤2‚ÄãD+œµ,Here, X is the continuous variable and D is the indicator variable.The introduction of D allows the model to estimate the difference in Y for the two categories of D, while also capturing the linear relationship between Y and the continuous variable X. For example, if D represents gender (male = 0, female = 1), and X is years of experience, the model will estimate the change in income with each additional year of experience, while also showing the difference in income between males and females.The model now reflects both the gradual change due to the continuous variable and the shift between groups due to the indicator variable, offering a more nuanced understanding of the relationship between the dependent and independent variables.Effect of Adding an Interaction Between a Continuous and an Indicator Variable in Multiple Linear Regression Models.Introducing an interaction term between a continuous variable and an indicator variable in MLR helps capture how the effect of one variable on Y changes depending on the value of the other variable. The model becomes:Y=Œ≤0‚Äã+Œ≤1‚ÄãX+Œ≤2‚ÄãD+Œ≤3‚Äã(X√óD)+œµ Where ùëã√óùê∑ is the interaction term.The interaction term ùõΩ3(ùëã√óùê∑)Œ≤3‚Äã(X√óD)indicates how the effect of the continuous variable X on ùëå differs depending on the value of D (the indicator variable). This allows for more complex relationships between the variables, such as different slopes for different categories of D.For example, in a model where X is age and D represents urban vs. rural (coded as 0 and 1), the interaction term would capture how the effect of age on income differs between urban and rural areas.Behavior of a Multiple Linear Regression Model Based Only on Indicator Variables from a Non-Binary Categorical Variable.When using only indicator variables derived from a non-binary categorical variable (i.e., more than two categories), the model will have multiple indicator variables, one for each category, excluding one as the reference category. The model takes the form:Y=Œ≤0‚Äã+Œ≤1‚ÄãD1‚Äã+Œ≤2‚ÄãD2‚Äã+‚ãØ+Œ≤n‚àí1‚ÄãDn‚àí1‚Äã+œµ\n",
    "Where ùê∑1,ùê∑2,‚Ä¶,ùê∑ùëõ‚àí1,D1‚Äã,D2‚Äã,‚Ä¶,Dn‚àí1‚Äãare the indicator variables for each category (e.g., regions or product types), and ùõΩ1,ùõΩ2,‚Ä¶,ùõΩùëõ‚àí1,Œ≤1‚Äã,Œ≤2‚Äã,‚Ä¶,Œ≤n‚àí1‚Äãrepresent the difference between each category and the reference category.\n",
    "  This type of model allows us to compare each category to the reference category, estimating how Y differs across the different levels of the categorical variable. The encoding of each category as a binary variable (0 or 1) allows the model to estimate the individual effect of each category.Example: If the categorical variable is \"region\" with three categories (North, South, West), the model will use two indicator variables (e.g., ùê∑1 for \"South\" and ùê∑2 for \"West\"), with the \"North\" region being the reference category. The model will estimate the difference in ùëå between the South and North, and the West and North.In summary, Multiple Linear Regression (MLR) extends Simple Linear Regression (SLR) by allowing for multiple independent variables, which helps model more complex relationships and improve accuracy. The addition of indicator variables and interaction terms allows MLR to capture categorical differences and more complex interactions, offering a richer and more detailed understanding of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc155a4",
   "metadata": {},
   "source": [
    "### 2. Explain in your own words (but working with a ChatBot if needed) what the specific (outcome and predictor) variables are for the scenario below; whether or not any meaningful interactions might need to be taken into account when predicting the outcome; and provide the linear forms with and without the potential interactions that might need to be considered<br>\n",
    "\n",
    "> Imagine a company that sells sports equipment. The company runs advertising campaigns on TV and online platforms. The effectiveness of the TV ad might depend on the amount spent on online advertising and vice versa, leading to an interaction effect between the two advertising mediums.    \n",
    "\n",
    "1. Explain how to use these two formulas to make **predictions** of the **outcome**, and give a high level explaination in general terms of the difference between **predictions** from the models with and without the **interaction** \n",
    "\n",
    "2. Explain how to update and use the implied two formulas to make predictions of the outcome if, rather than considering two continuous predictor variables, we instead suppose the advertisement budgets are simply categorized as either \"high\" or \"low\" (binary variables)    \n",
    "    \n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _While working on this question, it's important to clearly understand the (**outcome** and **predictor**) **variables** under consideration, and they way they are being considered. Similarly to the previous (first) question of this homework assignment, this question requires the **equations** of the indicated **linear forms** and an explanation of their **interpretation** and use. What is different here is that the **interactions** being considered are between two **continuous variables** or two **binary variables** (for a total of four **equations** under consideration where two include the **interactions** and two do not)._\n",
    ">\n",
    "> _The way an **interaction** actually appears and works in the context of the linear form of a **multiple linear regression** model may not necessarily be immediately intuitive, as it is actually in fact somewhat subtle and tricky. Of course, an **interaction** is when the relationship of one **predictor variable** with the **outcome variable** depends on the value of another different **predictor variable**, so the impact of one **predictor variable** changes based on the presence or magnitude of another **predictor variable**. But are you sure you know what this means in the context of the **linear form** of a **multiple linear regression** model?_\n",
    ">\n",
    "> - _Imagine you're selling smoothies, the taste of your smoothie depends on the ingredients included in the smoothie, and there are two ingredients (bananas and strawberries) in the smoothie._\n",
    ">     - _Adding more bananas into the smoothie will of course increase the \"banana\" flavor of the smoothie, and vice-versa for strawberries..._\n",
    ">     - _But is this \"banana\" influence on the flavor always constant, or can it change depending on the absolute amount of strawberries in the smoothie?_ \n",
    ">     - _If the \"banana\" flavor influence is constant and does not depend on the  absolute amount of strawberries in the smoothie, then there is no **interaction** and the **linear form** of the model is $\\beta_b b_i + \\beta_s s_i$ and the model is said to be only **additive**._\n",
    ">     - _But if the \"banana\" flavor influence does depend on the absolute amount of strawberries in the smoothie, then there IS an **interaction** and the **linear form** of the model is $\\beta_b b_i + \\beta_s s_i + \\beta_{bs} (b_i\\times s_i)$ and the model is said to be **synergistic**._\n",
    ">         \n",
    "> _These **linear forms** show that either bananas and strawberries do not have any **synergistic interaction** and contribute to the flavor independently; or, they do have a **synergistic interaction** and there is an interesting interplay between bananas and strawberries in the way they influence the taste of the smoothie._ \n",
    "> \n",
    "> - _So, if there is no **interaction**, then the effect of adding more bananas on the taste of the smoothie will always be the same, no matter how many strawberries you put in. So the effect of bananas on the smoothie is the same whether you add a lot of strawberries or just a few: $\\beta_b b_i + \\beta_s s_i$_\n",
    "> - _Or, on the other hand, if there is an **interaction**, then the effect of adding bananas (on the smootie flavor) will be different depending on how many strawberries there currently are in the smoothie: $\\beta_b b_i + \\beta_s s_i + \\beta_{bs} (b_i\\times s_i)$_\n",
    "> \n",
    "> _In this case, the right answer is probably that the **linear form** with the **interaction** is correct. This is because the flavor probably depends on the relative amount of bananas and strawberries in the smoothie; so, the effect of adding a fixed amount of bananas to the smoothie probalby depends on the absolute amount of strawberries that are in the smoothie._\n",
    "> \n",
    "> _Again, because understanding **interactions** in the context of **linear forms** is somewhat subtle and tricky and indeed not necessarily obviously intuitive, let's think about this a bit more. And we can simplify the concept a little bit by considering how this **interaction** would actually technically work in a **linear form** if we just had **binary indicator variables**._\n",
    ">         \n",
    "> - _To consider the smootie example in terms of binary variables, suppose that if both fruits are added to the smootie, they will be added in the same amount. So the smoothie will be made with either just bananas, just strawberries, or both (or neither and you won't make a smoothie)._ \n",
    ">     - _The question regarding an **interaction** then is, is the influence of the ingredients on the taste of the smoothie **additive** or **synergistic**? That is, does the way bananas affects the flavor of the smoothie change depending on the inclusion or exclusion of strawberries in the smoothie?_\n",
    ">     - _**Additive** $\\beta_b 1_{[b_i=1]}(b_i) + \\beta_s 1_{[s_i=1]}(s_i)$ means there are three different flavors but they are explained by just two **parameters**: banana $\\beta_b$, strawberry $\\beta_s$, and banana-strawberry $\\beta_b+\\beta_s$_\n",
    ">     - _**Synergistic** $\\beta_b 1_{[b_i=1]}(b_i) + \\beta_s 1_{[s_i=1]}(s_i) + \\beta_{bs}(1_{[b_i=1]}(b_i) \\times 1_{[s_i=1]}(s_i))$ means there are of course again three different flavors, but this time they are explained by three **parameters**: banana $\\beta_b$, strawberry $\\beta_s$, and banana-strawberry $\\beta_b+\\beta_s + \\beta_{bs}$, which indicates that the flavor is \"more than just sum of its parts\", meaning there is a **synergistic interaction** and there is an interesting interplay between bananas and strawberries in the way they influence the taste of the smoothie_\n",
    ">     \n",
    "> _As the **additive** and **synergistic** versions of the **linear form** of the two **binary indicator variables** context shows, we don't need an interaction to make different predictions for different combinations of things. Instead, what these show is that the prediction will either be **additive** and \"just the sum of it's parts\" or **synergistic** (**interactive**) and \"more than just sum of its parts\"._\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "      \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a908e784",
   "metadata": {},
   "source": [
    "1. Outcome and Predictor Variables:\n",
    "In this scenario, the outcome variable is sales, which represents the company's total sales as a result of the advertising efforts. The predictor variables are:\n",
    "\n",
    "TV Advertising spend: The amount of money spent on TV ads.\n",
    "Online Advertising spend: The amount of money spent on online ads (e.g., social media, online campaigns, etc.).\n",
    "These predictor variables are used to model how changes in advertising spending on TV and online platforms influence sales.\n",
    "\n",
    "2. Potential Interaction Effect:\n",
    "Given the problem statement, the effectiveness of TV ads might depend on how much is spent on online ads, and vice versa. This suggests that there is an interaction effect between TV and online advertising, meaning the combined effect of both types of ads might be different from the sum of their individual effects.\n",
    "\n",
    "Therefore, interaction terms should be considered in the model. An interaction term represents the combined influence of both predictor variables on the outcome variable (sales). This interaction effect would help capture scenarios where the effect of one advertising medium is influenced by the level of spending on the other medium.\n",
    "\n",
    "3. Linear Models With and Without the Interaction Term:\n",
    "Model Without the Interaction Term:The basic Multiple Linear Regression model without the interaction term is:\n",
    "Sales=Œ≤0‚Äã+Œ≤1‚Äã(TV¬†Advertising)+Œ≤2‚Äã(Online¬†Advertising)+œµ\n",
    "Œ≤0‚Äã: Represents the baseline sales when both TV and online advertising spending are zero.\n",
    "ùõΩ1‚Äã: The effect of TV advertising on sales, assuming online advertising is held constant.\n",
    "Œ≤2‚Äã: The effect of online advertising on sales, assuming TV advertising is held constant.\n",
    "œµ: The error term, capturing any variation not explained by the model.\n",
    "This model assumes that the effects of TV and online advertising are independent of each other ‚Äî meaning the impact of one type of advertising on sales does not change based on the amount spent on the other type.\n",
    "Model With the Interaction Term:\n",
    "To account for the potential interaction effect, we add an interaction term between TV and online advertising:\n",
    "Sales=Œ≤0‚Äã+Œ≤1‚Äã(TV¬†Advertising)+Œ≤2‚Äã(Online¬†Advertising)+Œ≤3‚Äã(TV¬†Advertising√óOnline¬†Advertising)+œµ\n",
    "ùõΩ3‚Äã: The coefficient for the interaction term, which represents how the effect of one advertising medium on sales changes depending on the amount spent on the other medium.\n",
    "If Œ≤3‚Äã>0: It suggests a positive interaction, meaning the combined effect of TV and online advertising leads to more sales than would be expected by simply adding their individual effects.\n",
    "If Œ≤3‚Äã<0: It indicates a negative interaction, where the combined effect of both ads is less than the sum of their individual effects.\n",
    "If Œ≤3‚Äã=0: No interaction effect is present, and the relationship between TV and online ads is additive.\n",
    "This model provides a more flexible approach by allowing for the possibility that TV and online advertising work together in a way that is not simply the sum of their separate impacts.\n",
    "\n",
    "4. Using Binary Variables Instead of Continuous Variables:\n",
    "If instead of continuous variables (i.e., actual amounts spent on TV and online ads), we categorize the advertising budgets as binary variables (e.g., \"high\" vs. \"low\" spending), the model would change to:\n",
    "\n",
    "Model With Binary Variables:\n",
    "Let:\n",
    "\n",
    "TV Advertising (binary): 0 = low spending, 1 = high spending.\n",
    "Online Advertising (binary): 0 = low spending, 1 = high spending.\n",
    "The Multiple Linear Regression model with binary predictors (and an interaction term) becomes:\n",
    "Sales=Œ≤0‚Äã+Œ≤1‚Äã(TV¬†Advertising)+Œ≤2‚Äã(Online¬†Advertising)+Œ≤3‚Äã(TV¬†Advertising√óOnline¬†Advertising)+œµùõΩ1‚Äã: The effect of increasing TV advertising from \"low\" to \"high\" on sales, assuming online advertising is held constant.\n",
    "Œ≤2‚Äã: The effect of increasing online advertising from \"low\" to \"high\" on sales, assuming TV advertising is held constant.\n",
    "Œ≤3‚Äã: The interaction effect between the two advertising types, representing how the impact of one advertising medium (e.g., TV) changes based on the level of the other (e.g., online).\n",
    "Differences Between Models With and Without Interaction:\n",
    "Model Without Interaction: Assumes that TV and online advertising influence sales independently. The effect of each advertising medium is constant and doesn't depend on the level of the other medium.\n",
    "Model With Interaction: Allows the relationship between TV and online advertising to vary depending on the level of the other type. This more complex model accounts for the possibility that the combined effect of the two types of advertising is different from the sum of their individual effects.\n",
    "Summary:\n",
    "In this scenario, you would use the model with the interaction term if you believe that the two advertising mediums influence sales together in a way that is more than just the sum of their separate effects. When switching to binary variables (high/low spending), the interpretation changes, but the interaction between the two advertising types is still important to capture how one affects the sales outcome when combined with the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f097af67",
   "metadata": {},
   "source": [
    "### 3. Use *smf* to fit *multiple linear regression* models to the course project dataset from the canadian social connection survey<br>\n",
    "\n",
    "> **EDIT: No, you probably actually care about CATEGORICAL or BINARY outcomes rather than CONTINUOUS outcomes... so you'll probably not actually want to do _multiple linear regression_ and instead do _logistic regression_ or _multi-class classification_. Okay, I'll INSTEAD guide you through doing _logistic regression_.**\n",
    "\n",
    "1. ~~for an **additive** specification for the **linear form** based on any combination of a couple **continuous**, **binary**, and/or **categorical variables** and a **CONTINUOUS OUTCOME varaible**~~ \n",
    "    1. This would have been easy to do following the instructions [here](https://www.statsmodels.org/dev/example_formulas.html). A good alternative analagous presentation for logistic regression I just found seems to be this one from a guy named [Andrew](https://www.andrewvillazon.com/logistic-regression-python-statsmodels/). He walks you through the `logit` alternative to `OLS` given [here](https://www.statsmodels.org/dev/api.html#discrete-and-count-models).\n",
    "    2. Logistic is for a **binary outcome** so go see this [piazza post](https://piazza.com/class/m0584bs9t4thi/post/346_f1) describing how you can turn any **non-binary categorical variable** into a **binary variable**. \n",
    "    3. Then instead do this problem like this: **catogorical outcome** turned into a **binary outcome** for **logistic regression** and then use any **additive** combination of a couple of **continuous**, **binary**, and/or **categorical variables** as **predictor variables**. \n",
    "\n",
    "\n",
    "```python\n",
    "# Here's an example of how you can do this\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "pokeaman = pd.read_csv(url).fillna('None')\n",
    "\n",
    "pokeaman['str8fyre'] = (pokeaman['Type 1']=='Fire').astype(int)\n",
    "linear_model_specification_formula = \\\n",
    "'str8fyre ~ Attack*Legendary + Defense*I(Q(\"Type 2\")==\"None\") + C(Generation)'\n",
    "log_reg_fit = smf.logit(linear_model_specification_formula, data=pokeaman).fit()\n",
    "log_reg_fit.summary()\n",
    "```\n",
    "\n",
    "\n",
    "2. ~~for a **synertistic interaction** specification for the **linear form** based on any combination of a couple **continuous**, **binary**, and/or **categorical variables**~~\n",
    "    1. But go ahead and AGAIN do this for **logistic regression** like above.\n",
    "    2. Things are going to be A LOT simpler if you restrict yourself to **continuous** and/or **binary predictor variables**.  But of course you could *use the same trick again* to treat any **categorical variable** as just a **binary variable** (in the manner of [that piazza post](https://piazza.com/class/m0584bs9t4thi/post/346_f1).\n",
    "    \n",
    "\n",
    "3. and **interpretively explain** your **linear forms** and how to use them to make **predictions**\n",
    "    1. Look, intereting **logistic regression** *IS NOT* as simple as interpreting **multivariate linear regression**. This is because it requires you to understand so-called **log odds** and that's a bit tricky. \n",
    "    2. So, INSTEAD, **just intepret you logistic regression models** *AS IF* they were **multivariate linear regression model predictions**, okay?\n",
    "\n",
    "\n",
    "4. and interpret the statistical evidence associated with the **predictor variables** for each of your model specifications \n",
    "    1. **Yeah, you're going to be able to do this based on the `.fit().summary()` table _just like with multiple linear regression_**... now you might be starting to see how AWESOME all of this stuff we're doing is going to be able to get...\n",
    "\n",
    "\n",
    "5. and finally use `plotly` to visualize the data with corresponding \"best fit lines\" for a model with **continuous** plus **binary indicator** specification under both (a) **additive** and (b) **synergistic** specifications of the **linear form** (on separate figures), commenting on the apparent necessity (or lack thereof) of the **interaction** term for the data in question\n",
    "    1. Aw, shit, you DEF not going to be able to do this if you're doing **logistic regression** because of that **log odds** thing I mentioned... hmm...\n",
    "    2. OKAY! Just *pretend* it's **multivariate linear regression** (even if you're doing **logistic regression**) and *pretend* your **fitted coefficients** belong to a **continuous** and a **binary predictor variable**; then, draw the lines as requested, and simulate **random noise** for the values of your **predictor data** and plot your lines along with that data.\n",
    "    \n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _This [link](https://www.statsmodels.org/dev/examples/notebooks/generated/formulas.html) offers guidance on using `statsmodels.formula.api` (`smf`) to build statistical models in Python using formulas._\n",
    ">\n",
    "> _The \"best fit lines\" summarize the relationship between the **outcome** and **predictor variables** observed in the data as well as the **linear form** of the **multiple linear regression** allows. The statistical evidence for the these estimated realtionship characterizations of course depends on an evaluation of the **hypothesis testing** for the **coefficients** of the model. **Model building** is the process of exploring the evidence for observed relationships captured through the modeling of the data in order to arrive at reliable (**generalizable**) claims based on the data, and perhaps make predictions about the future based on these created beliefs and understandings (whose value of course depends on how trustworthy these created beliefs and understandings are)._\n",
    ">\n",
    "> _When we do not find sufficient sufficient evidence for supposed relationships that we'd like to leverage for understanding or prediction, attempting to move forward on the basis of such \"findings\" is certainly a dangerous errand..._\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea0e2a",
   "metadata": {},
   "source": [
    "1. Perform Logistic Regression using smf in Python\n",
    "To conduct a logistic regression analysis on the dataset from the Canadian Social Connection Survey (or any other dataset), we need to use statsmodels.formula.api (commonly abbreviated as smf). In this case, the dependent variable should be binary (0 or 1), and the predictors can be a mix of continuous, binary, and categorical variables.\n",
    "\n",
    "Here‚Äôs how you can perform the logistic regression analysis:\n",
    "\n",
    "Step 1: Fit the Logistic Regression Model\n",
    "First, you‚Äôll need to import the necessary libraries and prepare your data. The Logit function from smf can be used to fit the logistic regression model. You will also define the formula specifying the outcome variable and the predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01f113b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.228109\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:               str8fyre   No. Observations:                  800\n",
      "Model:                          Logit   Df Residuals:                      788\n",
      "Method:                           MLE   Df Model:                           11\n",
      "Date:                Fri, 15 Nov 2024   Pseudo R-squ.:                 0.05156\n",
      "Time:                        14:39:19   Log-Likelihood:                -182.49\n",
      "converged:                       True   LL-Null:                       -192.41\n",
      "Covariance Type:            nonrobust   LLR p-value:                   0.04757\n",
      "============================================================================================================\n",
      "                                               coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                   -3.2644      0.714     -4.572      0.000      -4.664      -1.865\n",
      "Legendary[T.True]                            4.3478      2.179      1.996      0.046       0.078       8.618\n",
      "I(Q(\"Type 2\") == \"None\")[T.True]             1.5432      0.853      1.810      0.070      -0.128       3.215\n",
      "C(Generation)[T.2]                          -0.0574      0.468     -0.123      0.902      -0.975       0.861\n",
      "C(Generation)[T.3]                          -0.6480      0.466     -1.390      0.164      -1.561       0.265\n",
      "C(Generation)[T.4]                          -0.8255      0.545     -1.516      0.130      -1.893       0.242\n",
      "C(Generation)[T.5]                          -0.5375      0.449     -1.198      0.231      -1.417       0.342\n",
      "C(Generation)[T.6]                           0.3213      0.477      0.673      0.501      -0.614       1.257\n",
      "Attack                                       0.0172      0.006      3.086      0.002       0.006       0.028\n",
      "Attack:Legendary[T.True]                    -0.0365      0.019     -1.884      0.060      -0.074       0.001\n",
      "Defense                                     -0.0098      0.008     -1.247      0.213      -0.025       0.006\n",
      "Defense:I(Q(\"Type 2\") == \"None\")[T.True]    -0.0197      0.012     -1.651      0.099      -0.043       0.004\n",
      "============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Example dataset - you can replace this with the actual data\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "data = pd.read_csv(url).fillna('None')\n",
    "\n",
    "# Create a binary outcome variable\n",
    "data['str8fyre'] = (data['Type 1'] == 'Fire').astype(int)\n",
    "\n",
    "# Define the logistic regression formula\n",
    "# This includes continuous, binary, and categorical predictors\n",
    "formula = 'str8fyre ~ Attack * Legendary + Defense * I(Q(\"Type 2\") == \"None\") + C(Generation)'\n",
    "\n",
    "# Fit the logistic regression model\n",
    "log_reg_fit = smf.logit(formula, data=data).fit()\n",
    "\n",
    "# Print the model summary to see the results\n",
    "print(log_reg_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b877d97",
   "metadata": {},
   "source": [
    "Step 2: Interpreting the Logistic Regression Coefficients\n",
    "When interpreting the output from log_reg_fit.summary(), focus on the following elements:\n",
    "\n",
    "Intercept (Œ≤0): This is the log-odds of the outcome (e.g., str8fyre = 1) when all predictors are zero.\n",
    "Coefficients for Continuous Variables: The coefficients for continuous predictors (e.g., Attack, Defense) represent the change in the log-odds of the outcome for each one-unit increase in the predictor, holding other variables constant.\n",
    "The odds ratio is the exponentiation of the coefficient (exp(Œ≤)), which tells you the multiplicative change in the odds of the outcome for each one-unit increase in the predictor.\n",
    "Coefficients for Binary Categorical Variables: The coefficients for binary categorical variables (e.g., Legendary) represent the change in the log-odds of the outcome when the category changes from 0 to 1.\n",
    "If the coefficient for Legendary is positive, it means being a Legendary Pok√©mon increases the odds of being a Fire type.\n",
    "Example Output Interpretation:\n",
    "Assume the logistic regression output looks like this:\n",
    "\n",
    "Intercept (Œ≤0) = -1.5\n",
    "Coefficient for Attack (Œ≤1) = 0.02\n",
    "Coefficient for Legendary (Œ≤2) = -0.5\n",
    "Coefficient for Defense (Œ≤3) = 0.01\n",
    "Interpretation:\n",
    "\n",
    "Intercept: When all predictors are zero (e.g., no Attack, no Defense, not Legendary), the log-odds of the outcome (str8fyre = 1) is -1.5, and the odds are exp(-1.5) ‚âà 0.22.\n",
    "Attack: A one-unit increase in Attack increases the log-odds of the outcome by 0.02. The odds ratio is exp(0.02) ‚âà 1.02, meaning each additional unit of Attack slightly increases the odds of being a Fire type.\n",
    "Legendary: If the Pok√©mon is Legendary (Legendary = 1), the log-odds of being a Fire type decreases by 0.5. The odds ratio is exp(-0.5) ‚âà 0.61, suggesting that Legendary Pok√©mon are less likely to be of the Fire type compared to non-Legendary ones.\n",
    "Defense: A one-unit increase in Defense increases the log-odds of the outcome by 0.01, with an odds ratio of exp(0.01) ‚âà 1.01.\n",
    "2. Visualizing the Best-Fit Line with Plotly\n",
    "To visualize the logistic regression results, especially when dealing with continuous predictors, we can use Plotly to plot the observed data along with the predicted probabilities from the logistic regression model.\n",
    "\n",
    "Step 1: Create Prediction Data for Visualization\n",
    "To plot the best-fit line, we need to create a prediction dataset that spans the range of the continuous predictor variables (e.g., Attack), while keeping categorical variables constant (e.g., Legendary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e897ae8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'smf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m pred_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttack\u001b[39m\u001b[38;5;124m'\u001b[39m: attack_range,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLegendary\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Assuming Legendary = 0 (you can repeat this for Legendary = 1)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m })\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Add constant for the intercept term\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m pred_data \u001b[38;5;241m=\u001b[39m \u001b[43msmf\u001b[49m\u001b[38;5;241m.\u001b[39madd_constant(pred_data)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Make predictions using the fitted model\u001b[39;00m\n\u001b[1;32m     17\u001b[0m pred_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_Prob\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m log_reg_fit\u001b[38;5;241m.\u001b[39mpredict(pred_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'smf' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create a range for the continuous variable (e.g., Attack)\n",
    "attack_range = np.linspace(data['Attack'].min(), data['Attack'].max(), 100)\n",
    "\n",
    "# Create a DataFrame for predictions (e.g., keep Legendary = 0 and 1 for both cases)\n",
    "pred_data = pd.DataFrame({\n",
    "    'Attack': attack_range,\n",
    "    'Legendary': [0] * 100  # Assuming Legendary = 0 (you can repeat this for Legendary = 1)\n",
    "})\n",
    "\n",
    "# Add constant for the intercept term\n",
    "pred_data = smf.add_constant(pred_data)\n",
    "\n",
    "# Make predictions using the fitted model\n",
    "pred_data['Predicted_Prob'] = log_reg_fit.predict(pred_data)\n",
    "\n",
    "# Visualize the observed data and the predicted probabilities\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data['Attack'],\n",
    "    y=data['str8fyre'],\n",
    "    mode='markers',\n",
    "    name='Observed Data',\n",
    "    marker=dict(color=data['Legendary'], colorscale='Viridis', size=10)\n",
    "))\n",
    "\n",
    "# Add the best-fit line for the continuous predictor (Attack)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=pred_data['Attack'],\n",
    "    y=pred_data['Predicted_Prob'],\n",
    "    mode='lines',\n",
    "    name='Best-Fit Line',\n",
    "    line=dict(color='red')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Logistic Regression: Predicted Probabilities vs Attack',\n",
    "    xaxis_title='Attack',\n",
    "    yaxis_title='Predicted Probability'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0cb667",
   "metadata": {},
   "source": [
    "Step 2: Visualizing Interaction Effects (if applicable)\n",
    "If you have interaction terms in your model (e.g., Attack * Legendary), you can visualize how the relationship between Attack and the predicted probability of being a Fire type varies depending on whether the Pok√©mon is Legendary or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f721643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.626703\n",
      "         Iterations 5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (100,4) and (3,) not aligned: 4 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m pred_data_legendary_1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLegendary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Update Legendary to 1 for this dataset\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Get predicted probabilities for Legendary = 1\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m pred_data_legendary_1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_Prob\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mlog_reg_fit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_data_legendary_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Visualize the interaction effect: Plot the predicted probabilities for both Legendary = 0 and Legendary = 1\u001b[39;00m\n\u001b[1;32m     46\u001b[0m fig \u001b[38;5;241m=\u001b[39m go\u001b[38;5;241m.\u001b[39mFigure()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/statsmodels/base/model.py:1176\u001b[0m, in \u001b[0;36mResults.predict\u001b[0;34m(self, exog, transform, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;124;03mCall self.model.predict with self.params as the first argument.\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;124;03mreturned prediction.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m exog, exog_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_predict_exog(exog,\n\u001b[1;32m   1174\u001b[0m                                                 transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m-> 1176\u001b[0m predict_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m                                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exog_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(predict_results,\n\u001b[1;32m   1180\u001b[0m                                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_values\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m predict_results\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/statsmodels/discrete/discrete_model.py:543\u001b[0m, in \u001b[0;36mBinaryModel.predict\u001b[0;34m(self, params, exog, which, linear, offset)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    541\u001b[0m     exog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog\n\u001b[0;32m--> 543\u001b[0m linpred \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m offset\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m which \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcdf(linpred)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (100,4) and (3,) not aligned: 4 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Example data for fitting a logistic regression model:\n",
    "data = pd.DataFrame({\n",
    "    'Attack': [80, 85, 90, 95, 100, 110, 120, 125],\n",
    "    'Legendary': [0, 1, 0, 0, 1, 1, 0, 1],\n",
    "    'Type 1': ['Fire', 'Water', 'Fire', 'Electric', 'Fire', 'Water', 'Grass', 'Electric'],\n",
    "    'Type 2': ['Flying', 'None', 'None', 'None', 'Flying', 'None', 'Poison', 'None'],\n",
    "    'Result': [1, 0, 1, 0, 1, 1, 0, 1]\n",
    "})\n",
    "\n",
    "# Fit a logistic regression model (Example)\n",
    "X = data[['Attack', 'Legendary']]  # Predictor variables\n",
    "X = sm.add_constant(X)  # Adds intercept\n",
    "y = data['Result']  # Outcome variable (binary)\n",
    "\n",
    "# Fit the model\n",
    "log_reg_fit = sm.Logit(y, X).fit()\n",
    "\n",
    "# Now we will create the prediction data\n",
    "attack_range = np.linspace(data['Attack'].min(), data['Attack'].max(), 100)\n",
    "\n",
    "# Create a DataFrame for predictions (Legendary = 0 for all predictions)\n",
    "pred_data = pd.DataFrame({\n",
    "    'Attack': attack_range,\n",
    "    'Legendary': [0] * 100  # Starting with Legendary = 0 for all predictions\n",
    "})\n",
    "\n",
    "# Add a constant for the intercept term (this must match the original model's structure)\n",
    "pred_data = sm.add_constant(pred_data[['Attack', 'Legendary']])\n",
    "\n",
    "# Make predictions for Legendary = 0\n",
    "pred_data['Predicted_Prob'] = log_reg_fit.predict(pred_data)\n",
    "\n",
    "# Now, create the separate datasets for Legendary = 1\n",
    "pred_data_legendary_1 = pred_data.copy()\n",
    "pred_data_legendary_1['Legendary'] = 1  # Update Legendary to 1 for this dataset\n",
    "\n",
    "# Get predicted probabilities for Legendary = 1\n",
    "pred_data_legendary_1['Predicted_Prob'] = log_reg_fit.predict(pred_data_legendary_1)\n",
    "\n",
    "# Visualize the interaction effect: Plot the predicted probabilities for both Legendary = 0 and Legendary = 1\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot for Legendary = 0\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=pred_data['Attack'],\n",
    "    y=pred_data['Predicted_Prob'],\n",
    "    mode='lines',\n",
    "    name='Legendary = 0',\n",
    "    line=dict(color='blue')\n",
    "))\n",
    "\n",
    "# Plot for Legendary = 1\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=pred_data_legendary_1['Attack'],\n",
    "    y=pred_data_legendary_1['Predicted_Prob'],\n",
    "    mode='lines',\n",
    "    name='Legendary = 1',\n",
    "    line=dict(color='green')\n",
    "))\n",
    "\n",
    "# Update layout with titles and axis labels\n",
    "fig.update_layout(\n",
    "    title='Interaction Effect: Attack and Legendary Status',\n",
    "    xaxis_title='Attack',\n",
    "    yaxis_title='Predicted Probability'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb3f53",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "Logistic Regression Model: We fit a logistic regression model using statsmodels with continuous, binary, and categorical predictors. The coefficients provide insights into the relationship between predictors and the binary outcome, which is represented in terms of log-odds and odds ratios.\n",
    "\n",
    "Visualization: Using Plotly, we visualized the best-fit line for a continuous predictor (Attack), and also demonstrated how interaction effects between Attack and Legendary (a categorical variable) can be visualized using separate curves.\n",
    "\n",
    "This approach helps in understanding how each variable affects the outcome and visualizes the relationship clearly, while also interpreting the significance of interaction terms in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ac555",
   "metadata": {},
   "source": [
    "### 4. Explain the apparent contradiction between the factual statements regarding the fit below that \"the model only explains 17.6% of the variability in the data\" while at the same time \"many of the *coefficients* are larger than 10 while having *strong* or *very strong evidence against* the *null hypothesis* of 'no effect'\"<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> _How do we simultaneously interpret **hypothesis testing** results regarding **coefficient estimates** based on **p-values** and **R-squared** \"the proportion of variation in (outcome) $y$ explained by the model ($\\hat y_i$)\"? How can both be meaningfully understood at the same time? Do they address different aspects of a model?_\n",
    ">    \n",
    "> _As introduced in the previous homework, **R-squared** is_\n",
    ">\n",
    "> _$$R^2 = 1 - \\frac{\\sum_{i=1}^n(y_i-\\hat y)^2}{\\sum_{i=1}^n(y_i-\\bar y)^2}$$_\n",
    ">    \n",
    "> _which describes the **explanatory power** of a model; whereas, **p-values** allow us to characterize **evidence against** a **null hypothesis**, and **coefficients** in a **multiple linear regression** context allow us to interpret the relationship between the **outcome** and a **predictor variable** \"with all other **predictor variables** 'held constant'\". Are these concepts thus contradictory or conflictual in some manner?_\n",
    "\n",
    "|p-value|Evidence|\n",
    "|-|-|\n",
    "|$$p > 0.1$$|No evidence against the null hypothesis|\n",
    "|$$0.1 \\ge p > 0.05$$|Weak evidence against the null hypothesis|\n",
    "|$$0.05 \\ge p > 0.01$$|Moderate evidence against the null hypothesis|\n",
    "|$$0.01 \\ge p > 0.001$$|Strong evidence against the null hypothesis|\n",
    "|$$0.001 \\ge p$$|Very strong evidence against the null hypothesis|\n",
    "    \n",
    "> _In `formula='HP ~ Q(\"Sp. Def\") * C(Generation)'` the `Q` stands for \"quote\" and is needed to access column names when they have a \"space\" in their name, while the `C` indicates a **categorical** use of what is actually an **integer** valued column. Despite technically being **continuous** numbers, **integer** often simply indicate categories which should not necessarily be treated as an incremental **continuous predictor variable**. Remember, a model such as $\\beta_0 + \\beta_1 x$ means for each unit increase in $x$ the outcome increases \"on average\" by $\\beta_1$; so, if $x$ takes on the values `1` through `6` as the `Generation` **predictor variable** here does, then this means the average value for \"Generation 1\" must be $\\beta_0 + \\beta_1$ while for \"Generation 2\" it must be $\\beta_0 + 2\\times \\beta_1$ (and so on up to \"Generation 6\" which must be $\\beta_0 + 6\\times \\beta_1$). This might be a very strange restriction to place on something that is really actually a **categorical predictor variable**. You can see in the given model fit below how this six-level **categorical predictor variable** is actually appropriately treated in the specification of the **linear form** using \"Generation 1\" for the \"baseline\" and **binary indicators** to model the \"contrast\" (\"offsets\") for the other \"Generations\"; and, how these are in turn used in the context of the **interaction** considered by the model specification._ \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ccce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "# fail https://github.com/KeithGalli/pandas/blob/master/pokemon_data.csv\n",
    "pokeaman = pd.read_csv(url) \n",
    "pokeaman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a80b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model1_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation) + Q(\"Sp. Def\"):C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") * C(Generation)', data=pokeaman)\n",
    "\n",
    "model2_fit = model2_spec.fit()\n",
    "model2_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9542bb4",
   "metadata": {},
   "source": [
    "When a model explains only a small portion of the variability (e.g., 17.6%) but still shows significant coefficients with p-values < 0.05, it typically indicates that:\n",
    "\n",
    "Low R-squared: The model doesn‚Äôt capture much of the total variability in the outcome, possibly due to missing important variables or other unmeasured factors.\n",
    "\n",
    "Significant Predictors: Despite the low R-squared, individual predictors can still be significant, meaning they reliably affect the outcome, even if their effect is small.\n",
    "\n",
    "Large Coefficients: High coefficients (e.g., >10) may reflect the scale of the variables, not necessarily their practical impact. This can also happen if the predictors are not standardized.\n",
    "\n",
    "Sample Size: A large sample size can lead to statistically significant results for even small effects, which may not explain much variability in practice.\n",
    "\n",
    "Overall, a low R-squared value combined with significant coefficients suggests that while the predictors influence the outcome, there are other factors or complexities not captured by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614675a5",
   "metadata": {},
   "source": [
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Continue now...?</u></summary>\n",
    "\n",
    "### Pre-lecture VS Post-lecture HW\n",
    "    \n",
    "Feel free to work on the \"Post-lecture\" HW below if you're making good progress and want to continue: in this case the \"Post-lecture\" HW just builds on the \"Post-lecture\" HW, introducing and extending the considerations available in the **multiple linear regression context**. That said, as \"question 3\" above hopefully suggests and reminds you, the **course project** is well upon us, and prioritizing work on that (even over the homework) may very well be indicated at this point...\n",
    "\n",
    "*The benefits of continue would are that (a) it might be fun to try to tackle the challenge of working through some problems without additional preparation or guidance; and (b) this is a very valable skill to be comfortable with; and (c) it will let you build experience interacting with ChatBots (and beginning to understand their strengths and limitations in this regard)... it's good to have sense of when using a ChatBot is the best way to figure something out, or if another approach (such as course provided resources or a plain old websearch for the right resourse) would be more effective*\n",
    "    \n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a392bc2",
   "metadata": {},
   "source": [
    "## \"Post-lecture\" HW [*submission along with \"Pre-lecture\" HW is due prior to next TUT*]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51edb40a",
   "metadata": {},
   "source": [
    "### 5. Discuss the following (five cells of) code and results with a ChatBot and based on the understanding you arrive at in this conversation explain what the following (five cells of) are illustrating<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> _Recall from the previous week's homework that the **R-squared** \"the proportion of variation in (outcome) $y$ explained by the model ($\\hat y_i$)\" quantity (re-introduced in the previous problem) can be calculated as `np.corrcoef(y,fitted_model.fittedvalues)[0,1]**2` (as well as several other ways in the **simple linear regression** context). The **squared correlation** between the outcome $y$ and it's **fitted values** $\\hat y$ is the most generally useful formulation of **R-squared** since this can be use in the **multiple linear regression** context._\n",
    "> \n",
    "> _This question therefore thus addresses the question of model **generalizability** on the basis of \"in sample\" and \"out of sample\" **model performance** (measured by **R-squared**)._\n",
    "> \n",
    "> - _The **squared correlation** between the **outcomes** $y$ and their **fitted values** $\\hat y$ is an \"in sample\" **model performance** metric since the $\\hat y$ \"predictions\" for the $y$ **outcomes** are based on using those already **observed outcomes** to fit the model to generate the $\\hat y$._  \n",
    "> \n",
    "> - _If we instead calculate **squared correlation** between **outcomes** $y$ that were not used to fit the model and their corresponding $\\hat y$ **predictions** (which are indeed now actually **predictions** as opposed to **fitted values**), then we are now  calculating an \"out of sample\" **model performance** metric._\n",
    "> \n",
    "> _When an \"out of sample\" metric performs more poorly than a comparitive \"in sample\" metric, then the **predictions** of the **fitted model** are not **generalizing** to data being the dataset the model is fit on. In this case we say the model is **overfit** (to the data its fit was based on). The purpose of using different **training** and **testing** datasets is to consider \"in sample\" versus \"out of sample\" **model performance** in order to try to confirm that the model is not **overfit** and that the **predictions** do indeed seem to **generalizable** beyond the dataset used for **model fitting**._\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dd90ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
    "\n",
    "# Replace \"NaN\" (in the \"Type 2\" column with \"None\")\n",
    "pokeaman.fillna('None', inplace=True)\n",
    "\n",
    "np.random.seed(130)\n",
    "pokeaman_train,pokeaman_test = \\\n",
    "  train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n",
    "pokeaman_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a8b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec3 = smf.ols(formula='HP ~ Attack + Defense', \n",
    "                      data=pokeaman_train)\n",
    "model3_fit = model_spec3.fit()\n",
    "model3_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac52c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_model3 = model3_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model3_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model3)[0,1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfa038c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary'\n",
    "model4_linear_form += ' * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "# DO NOT try adding '* C(Generation) * C(Q(\"Type 1\")) * C(Q(\"Type 2\"))'\n",
    "# That's 6*18*19 = 6*18*19 possible interaction combinations...\n",
    "# ...a huge number that will blow up your computer\n",
    "\n",
    "model4_spec = smf.ols(formula=model4_linear_form, data=pokeaman_train)\n",
    "model4_fit = model4_spec.fit()\n",
    "model4_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d225cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_model4 = model4_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model4_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model4)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5497b4d4",
   "metadata": {},
   "source": [
    "Explanation of the Models and Results:\n",
    "Model Purpose:\n",
    "\n",
    "First Model: A simple linear regression with main effects (Attack and Defense) to understand their individual impact on HP.\n",
    "Second Model: Includes interaction terms between predictors (e.g., Attack, Defense, Speed, Legendary, etc.) to capture combined effects on HP, providing a more complex model.\n",
    "In-Sample and Out-of-Sample R2:\n",
    "\n",
    "In-Sample \n",
    "R2: Measures how well the model fits the training data. A higher value indicates the model explains more of the training data's variability.\n",
    "Out-of-Sample \n",
    "\n",
    "R2: Measures the model's ability to generalize to unseen data (test set). If this value is much lower than the in-sample \n",
    "R2, it suggests overfitting.\n",
    "Including Interaction Terms:\n",
    "\n",
    "Adding interaction terms allows the model to capture more complex relationships between predictors, which may improve predictive performance if these interactions are real.\n",
    "However, it also increases model complexity and may lead to overfitting, especially with too many interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deecc5b7",
   "metadata": {},
   "source": [
    "### 6. Work with a ChatBot to understand how the *model4_linear_form* (*linear form* specification of  *model4*) creates new *predictor variables* as the columns of the so-called \"design matrix\" *model4_spec.exog* (*model4_spec.exog.shape*) used to predict the *outcome variable*  *model4_spec.endog* and why the so-called *multicollinearity* in this \"design matrix\" (observed in *np.corrcoef(model4_spec.exog)*) contribues to the lack of \"out of sample\" *generalization* of *predictions* from *model4_fit*; then, explain this consisely in your own works<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _The **overfitting** observed in the previous problem is a question of **model complexity** relative to the amount of information or evidence available in a given dataset (or we could just say \"the amount of data in a dataset\"). The **model fit** for  `model4` resulted in an **overfit model** because the specification of its **linear form** was overly complex (relative to the the amount of available data). Indeed, `model4` is rediculously complex as can be seen from `model4_fit.summary()`. This in turn allowed the **model fit** to \"detect\" idiosyncratic associations spuriously present specifically in the **training** dataset but which did not **generalize** to the **testing** dataset. If a model is too **complex** then it will find and model \"patterns\" in a **training** dataset which are actually just accidental \"noise\" from the random sampling nature of the dataset. The simpler (or more parsimoneous) `model3` on the other hand was able to identify **predictive associations** in the **training** dataset which did **generalize** to the **testing** dataset. This is because `model3` only sought to understand simpler **predictive association** for which there was enough data in the **training** dataset to reliably detect and obviously identify. And these patterns were indeed sufficiently \"real\" in the sense that they were present and **generalized** into the **testing** dataset as well as the **training** dataset. So they could be \"found\" in the **training** and then used in (**generalized** to) the **testing** dataset._\n",
    "> \n",
    "> _This question therefore addresses the topic of the **evidence** a given dataset provides for the **predictive associations** detected by a **fitted model**. It should be increasingly clear at this point that evidence for a model can be addressed using **coefficent hypothesis testing** in the context of **multiple linear regression**, but that examinations of \"in sample\" versus \"out of sample\" **model performance** metrics are what in fact are directly designed to address this question of **generalizability**. That said, this question introduces another consideration of **multicollinearity** as something that affects the **generalizability** of **model fits** in the **multiple linear regression** context. A good question that a ChatBot could help you understand is (a) \"why is **generalizability** more uncertain if two **predictor variables** are highly **correlated**?\" and (b) \"why is **generalizability** more uncertain if multiple **predictor variables** are highly **multicollinear**?\"_\n",
    ">\n",
    "> _The four code cells below are not necessary for answering this question; however, they introduce two very practical helpful tools for the **multiple linear regression** context that are immediately relevant for this question. The first is the so-called **condition number** (of a \"design matrix\") which provides a very simple diagnostic which can serve as a measure the degree of **multicollinearity** that is present in a **model fit**. If this number is \"very large\" then there is a large degree of **multicollinearity** and suggests room for doubt regarding the **generalizability** of the **fitted model**. The second tool(s) are the `center` and `scale` functions. It is best practice to \"center and scale\" **continuous predictor variables** (but not **indicator variables**) in the **multiple linear regression** context as is done below. While \"centering and scaling\" does make interpreting the predictions on the original scale of the data slighly more complicated, it also must be done in order to get a \"true\" evaluation of the degree of **multicollinearity** present in a **model fit** using the **condition number** of the model (\"design matrix\"). The examples below show that the **condition number** reported by a **fitted model** are \"artificially inflacted\" if \"centering and scaling\" is not used. Specically, they show that the **condition number** of `model3_fit` is really `1.66` (as opposed to the \"very large\" `343` which is reported without \"centering and scaling\"); whereas, the **condition number** for `model4_fit` is \"very (VERY) large\" irrespective of \"centering and scaling\", showing that the overwheling presense of **multicollinearity** in `model4_fit` is in fact a very real thing.  Indeed, we have already seen that `model4_fit` is grossly **overfit** and does not remotely **generalize** beyond its **training** dataset. Without knowing this, however, the comically large **condition number** for `model4_fit` (after \"centering and scaling\") makes it abundantly clear that we should have great doubts about the likely **generalizability** of `model4_fit` (even without examining specific aspects of **multicollinearity** directly or examining \"in sample\" versus \"out of sample\" **model performance** comparisions)._\n",
    ">\n",
    "> - _The \"specific aspects of **multicollinearity**\" reference above refer to understanding and attributing the detrmimental affects of specific **predictor variables** towards **multicollinearity**. This can be done using so-called **variance inflation factors**, but this is beyond the scope of STA130. We should be aware that the presence of excessive **multicollinearity** as indicated by a large **condition number** for a (\"centered and scaled\") **fitted model** raises grave concerns regarding the potential **generalizability** of the model._\n",
    ">\n",
    "> _The `np.corrcoef(model4_spec.exog)` examination of the **correlations** of a \"design matrix\" considered in ths problems prompt is analogous to the examination of the **correlations** present in a dataset that might considered when initially examining the **predictor variables** of a dataset, such as `pokeaman.iloc[:,4:12].corr()`. Indeed, such an examination is often the first step in examining the potential presence of **multicollinearity** among the **predictor variables** of a dataset. However, these are consideration of **pairwise correlation**, whereas **multicollinearity** generalizes this notion to the full collection of **predictor variables** together. A **condition number** for a \"centered and scale\" version of a **fit model** can therefore be viewed as serving the analogous purposes of a multivariate generalization of **pairwise correlation**._\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Cond. No.\" WAS 343.0 WITHOUT to centering and scaling\n",
    "model3_fit.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedf01b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from patsy import center, scale\n",
    "\n",
    "model3_linear_form_center_scale = \\\n",
    "  'HP ~ scale(center(Attack)) + scale(center(Defense))' \n",
    "model_spec3_center_scale = smf.ols(formula=model3_linear_form_center_scale,\n",
    "                                   data=pokeaman_train)\n",
    "model3_center_scale_fit = model_spec3_center_scale.fit()\n",
    "model3_center_scale_fit.summary()\n",
    "# \"Cond. No.\" is NOW 1.66 due to centering and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb64e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Defense))'\n",
    "model4_linear_form_CS += ' * scale(center(Speed)) * Legendary' \n",
    "model4_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "# Legendary is an indicator, so we don't center and scale that\n",
    "\n",
    "model4_CS_spec = smf.ols(formula=model4_linear_form_CS, data=pokeaman_train)\n",
    "model4_CS_fit = model4_CS_spec.fit()\n",
    "model4_CS_fit.summary().tables[-1]  # Cond. No. is 2,250,000,000,000,000\n",
    "\n",
    "# The condition number is still bad even after centering and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f19d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just as the condition number was 5. Discuss the following (five cells of) code and results with a ChatBot and based on the understanding you arrive at in this conversation explain what the following (five cells of) are illustrating\n",
    "Further Guidance\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "‚Äã\n",
    "fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
    "‚Äã\n",
    "# Replace \"NaN\" (in the \"Type 2\" column with \"None\")\n",
    "pokeaman.fillna('None', inplace=True)\n",
    "‚Äã\n",
    "np.random.seed(130)\n",
    "pokeaman_train,pokeaman_test = \\\n",
    "  train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n",
    "pokeaman_train\n",
    "‚Äã\n",
    "model_spec3 = smf.ols(formula='HP ~ Attack + Defense', \n",
    "                      data=pokeaman_train)\n",
    "model3_fit = model_spec3.fit()\n",
    "model3_fit.summary()\n",
    "yhat_model3 = model3_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model3_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model3)[0,1]**2)\n",
    "model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary'\n",
    "model4_linear_form += ' * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "# DO NOT try adding '* C(Generation) * C(Q(\"Type 1\")) * C(Q(\"Type 2\"))'\n",
    "# That's 6*18*19 = 6*18*19 possible interaction combinations...\n",
    "# ...a huge number that will blow up your computer\n",
    "‚Äã\n",
    "model4_spec = smf.ols(formula=model4_linear_form, data=pokeaman_train)\n",
    "model4_fit = model4_spec.fit()\n",
    "model4_fit.summary()\n",
    "yhat_model4 = model4_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model4_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model4)[0,1]**2)\n",
    "6. Work with a ChatBot to understand how the model4_linear_form (linear form specification of model4) creates new predictor variables as the columns of the so-called \"design matrix\" model4_spec.exog (model4_spec.exog.shape) used to predict the outcome variable model4_spec.endog and why the so-called multicollinearity in this \"design matrix\" (observed in np.corrcoef(model4_spec.exog)) contribues to the lack of \"out of sample\" generalization of predictions from model4_fit; then, explain this consisely in your own works\n",
    "Further Guidance\n",
    "# \"Cond. No.\" WAS 343.0 WITHOUT to centering and scaling\n",
    "model3_fit.summary() \n",
    "from patsy import center, scale\n",
    "‚Äã\n",
    "model3_linear_form_center_scale = \\\n",
    "  'HP ~ scale(center(Attack)) + scale(center(Defense))' \n",
    "model_spec3_center_scale = smf.ols(formula=model3_linear_form_center_scale,\n",
    "                                   data=pokeaman_train)\n",
    "model3_center_scale_fit = model_spec3_center_scale.fit()\n",
    "model3_center_scale_fit.summary()\n",
    "# \"Cond. No.\" is NOW 1.66 due to centering and scaling\n",
    "model4_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Defense))'\n",
    "model4_linear_form_CS += ' * scale(center(Speed)) * Legendary' \n",
    "model4_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "# Legendary is an indicator, so we don't center and scale that\n",
    "‚Äã\n",
    "model4_CS_spec = smf.ols(formula=model4_linear_form_CS, data=pokeaman_train)\n",
    "model4_CS_fit = model4_CS_spec.fit()\n",
    "model4_CS_fit.summary().tables[-1]  # Cond. No. is 2,250,000,000,000,000\n",
    "‚Äã\n",
    "# The condition number is still bad even after centering and scaling\n",
    "# Just as the condition number was very bad to start with\n",
    "model4_fit.summary().tables[-1]  # Cond. No. is 12,000,000,000,000,000\n",
    "‚Äã\n",
    "7. Discuss with a ChatBot the very bad to start with\n",
    "model4_fit.summary().tables[-1]  # Cond. No. is 12,000,000,000,000,000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74edb90",
   "metadata": {},
   "source": [
    "Condition Number and Multicollinearity:\n",
    "Condition Number: It measures how sensitive a regression model is to changes in input data. A high condition number indicates potential multicollinearity, where predictors are highly correlated, making the model's coefficients unstable and sensitive to small data variations. This undermines the reliability of the model and its ability to generalize to new data.\n",
    "\n",
    "Multicollinearity: Occurs when predictors are highly correlated, making it hard for the model to distinguish their individual effects on the outcome. It inflates coefficient variances, leading to less reliable estimates and higher standard errors.\n",
    "\n",
    "Impact on Generalization:\n",
    "High multicollinearity often results in overfitting, where the model fits the training data well but performs poorly on new data (low out-of-sample \n",
    "ùëÖ\n",
    "2\n",
    "R \n",
    "2\n",
    " ). This happens because the model captures noise rather than the true underlying signal.\n",
    "Solutions:\n",
    "Centering and Scaling: Standardizing the predictors can help reduce the condition number, but it may not fully resolve multicollinearity if it‚Äôs deeply embedded.\n",
    "Regularization: Techniques like Ridge regression can reduce the effect of multicollinearity by penalizing large coefficients.\n",
    "Feature Selection: Removing highly correlated predictors can reduce multicollinearity and improve model stability.\n",
    "Conclusion:\n",
    "A high condition number suggests multicollinearity, which can make the model overly sensitive to changes and lead to poor generalization. Techniques like centering, scaling, regularization, and feature selection are key to addressing this issue and improving model performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a577c7dd",
   "metadata": {},
   "source": [
    "### 7. Discuss with a ChatBot the rationale and principles by which *model5_linear_form* is  extended and developed from *model3_fit* and *model4_fit*; *model6_linear_form* is  extended and developed from *model5_linear_form*; and *model7_linear_form* is  extended and developed from *model6_linear_form*; then, explain this breifly and consisely in your own words<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _We again include the **condition number** for the \"centered and scaled\" version of `model7_fit` to show that **multicollinearity** does not appear to be a major concern for this model (and the same would be true regarding `model6_fit` if the analogous \"centered and scaled\" version of the model was considered). While it is true that the **condition number** of `15.4` observed for `model7_fit` is perhaps \"large\", this would not be considered \"vary large\"._\n",
    ">\n",
    "> - _Regarding **condition numbers**, a ChatBot gave me cutoffs of `<30` not a big problem, up to `<300` maybe an issue, up to `<1000` definitely **multicollinearity**, and beyond that is pretty much likely to be \"serious\" problems with **multicollinearity**. Personally, cutoffs around `10`, `100`, and `1000` seem about right to me._\n",
    ">\n",
    "> _This question addresses the **model building** exercise using both an **evidence** based approach using **coefficient hypothesis testing** as well as examinations of **generalizability** using comparisions of \"in sample\" versus \"out of sample\" **model performance** metrics. Through these tools, different models were considered, extended, and developed, finally arriving at `model7_fit`. When we feel we can improve the **model performance** in a **generalizable** manner, then all relatively underperforming models are said to be **underfit**, meaning that they do not leverage all the **predictive associations** available to improve **predictions**._\n",
    "> \n",
    "> _While the previous \"Question 6\" above introduced and explored the impact of **multicollinearity** in the **multiple linear regression** context_ \n",
    ">     \n",
    "> - _(whereby \"the effects\" of multiple **predictor variables** are \"tangled up\" and therefore do not allow the model to reliably determine contribution attributions between the **predictor variables**, which potentially leads to poor **estimation** of their \"effects\" in the model, which in turn is the problematic state of affairs which leads to a lack of **generalizability** in such high **multicollinearity** settings)_\n",
    "> \n",
    "> _there is still the (actually even more important) consideration of the actual **evidence** of **predictive associations**. The question is whether or not there is sufficient **evidence** in the data backing up the **estimated** fit of the **linear form** specification. Quantifying the **evidence** for a **estimated** model is a separate question from the problem of **multicollinearity**, the assessment of which is actually the primary purpose of **multiple linear regression** methodology._\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5486d44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here's something a little more reasonable...\n",
    "model5_linear_form = 'HP ~ Attack + Defense + Speed + Legendary'\n",
    "model5_linear_form += ' + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "model5_linear_form += ' + C(Generation) + C(Q(\"Type 1\")) + C(Q(\"Type 2\"))'\n",
    "\n",
    "model5_spec = smf.ols(formula=model5_linear_form, data=pokeaman_train)\n",
    "model5_fit = model5_spec.fit()\n",
    "model5_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0dc7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_model5 = model5_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model5_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model5)[0,1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea79d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's something a little more reasonable...\n",
    "model6_linear_form = 'HP ~ Attack + Speed + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "# And here we'll add the significant indicators from the previous model\n",
    "# https://chatgpt.com/share/81ab88df-4f07-49f9-a44a-de0cfd89c67c\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model6_linear_form += ' + I(Generation==2)'\n",
    "model6_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model6_spec = smf.ols(formula=model6_linear_form, data=pokeaman_train)\n",
    "model6_fit = model6_spec.fit()\n",
    "model6_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91abe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_model6 = model6_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a2353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here's a slight change that seems to perhaps improve prediction...\n",
    "model7_linear_form = 'HP ~ Attack * Speed * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form += ' + I(Generation==2)'\n",
    "model7_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model7_spec = smf.ols(formula=model7_linear_form, data=pokeaman_train)\n",
    "model7_fit = model7_spec.fit()\n",
    "model7_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b677e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_model7 = model7_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeb1ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here's a slight change that seems to perhas improve prediction...\n",
    "model7_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Speed))'\n",
    "model7_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "# We DO NOT center and scale indicator variables\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form_CS += ' + I(Generation==2)'\n",
    "model7_linear_form_CS += ' + I(Generation==5)'\n",
    "\n",
    "model7_CS_spec = smf.ols(formula=model7_linear_form_CS, data=pokeaman_train)\n",
    "model7_CS_fit = model7_CS_spec.fit()\n",
    "model7_CS_fit.summary().tables[-1] \n",
    "# \"Cond. No.\" is NOW 15.4 due to centering and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821a61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Cond. No.\" WAS 2,340,000,000 WITHOUT to centering and scaling\n",
    "model7_fit.summary().tables[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a398bea",
   "metadata": {},
   "source": [
    "Model Progression and Rationale:\n",
    "Model3 (model3_fit):\n",
    "\n",
    "Purpose: Establishes a baseline model using main predictors (e.g., Attack, Defense) to predict the target variable HP.\n",
    "Significance: Provides a simple model for comparison and lays the foundation for further complexity.\n",
    "Model4 (model4_fit):\n",
    "\n",
    "Enhancement: Introduces interaction terms (e.g., Attack * Defense) to capture joint effects of predictors.\n",
    "Rationale: Helps uncover complex relationships between variables that aren't explained by individual terms alone.\n",
    "Model5 (model5_linear_form):\n",
    "\n",
    "Expansion: Adds more predictors and interactions (e.g., Speed, Legendary, and Type variables).\n",
    "Goal: Improves model flexibility by incorporating additional relevant variables, although this increases the risk of overfitting.\n",
    "Model6 (model6_linear_form):\n",
    "\n",
    "Refinement: Incorporates higher-order interactions and domain-specific features (e.g., specific Pok√©mon Type categories).\n",
    "Purpose: Further enhances the model‚Äôs ability to capture complex patterns, though care is needed to avoid overfitting.\n",
    "Model7 (model7_linear_form):\n",
    "\n",
    "Final Adjustments: Includes centering and scaling of predictors to address multicollinearity and improve numerical stability.\n",
    "Impact: Reduces the condition number, making the model more stable and interpretable, and enhances its ability to generalize to unseen data.\n",
    "Key Improvements:\n",
    "Interactions and Higher-Order Terms: Help capture more nuanced relationships between predictors and the outcome.\n",
    "Centering and Scaling: Stabilizes coefficients by addressing multicollinearity and ensures more reliable, interpretable results.\n",
    "In summary, each model builds on the previous one by adding complexity (more interactions, non-linear terms) and improving stability through centering and scaling, leading to better predictive performance and more reliable generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbbf01d",
   "metadata": {},
   "source": [
    "### 8. Work with a ChatBot to write a *for* loop to create, collect, and visualize many different paired \"in sample\" and \"out of sample\" *model performance* metric actualizations (by not using *np.random.seed(130)* within each loop iteration); and explain in your own words the meaning of your results and purpose of this demonstration<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> _The following code could likely be slightly edited and repurposed to match the 50-50 **train-test split** analysis and data above (in the `train_test_split` method of \"Question 5\").  Considering the behavior of the `model3_fit` **linear form** specification would be the suggested way to start, but it might also be interesting and/or helpful to consider the different available **linear form** specifications in the manner of this problem..._\n",
    ">    \n",
    "> ```python\n",
    "> import plotly.express as px  # etc.\n",
    ">\n",
    "> songs_training_data,songs_testing_data = train_test_split(songs, train_size=31)\n",
    "> linear_form = 'danceability ~ energy * loudness + energy * mode'\n",
    ">    \n",
    "> reps = 100\n",
    "> in_sample_Rsquared = np.array([0.0]*reps)\n",
    "> out_of_sample_Rsquared = np.array([0.0]*reps)\n",
    "> for i in range(reps):\n",
    ">     songs_training_data,songs_testing_data = \\\n",
    ">       train_test_split(songs, train_size=31)\n",
    ">     final_model_fit = smf.ols(formula=linear_form, \n",
    ">                               data=songs_training_data).fit()\n",
    ">     in_sample_Rsquared[i] = final_model_fit.rsquared\n",
    ">     out_of_sample_Rsquared[i] = \\\n",
    ">       np.corrcoef(songs_testing_data.danceability, \n",
    ">                   final_model_fit.predict(songs_testing_data))[0,1]**2\n",
    ">     \n",
    "> df = pd.DataFrame({\"In Sample Performance (Rsquared)\": in_sample_Rsquared,\n",
    ">                    \"Out of Sample Performance (Rsquared)\": out_of_sample_Rsquared})   >  \n",
    "> fig = px.scatter(df, x=\"In Sample Performance (Rsquared)\", \n",
    ">                      y=\"Out of Sample Performance (Rsquared)\")\n",
    "> fig.add_trace(go.Scatter(x=[0,1], y=[0,1], name=\"y=x\", line_shape='linear'))  \n",
    "> ```\n",
    ">\n",
    "> _When you first look at this question, you might be unsure about the specific issue that the code is addressing. Take a moment to think about why the code repeatedly randomly re-splits the data, fits the model, and compares the \"in sample\" versus \"out of sample\" **R-squared** values (over and over). Of course, if a **fit model** performs well on the **training** dataset but doesn't do as well on the **testing** dataset then we might be observing the affects of **overfitting**. But why might it sometimes be the opposite situation (which we actually encountered right away for `model3_fit` when the **train-test split** was based on  `np.random.seed(130)` and resulted in a better \"out of sample\" **R-squared** of about `0.21` vereses the 'in-sample\" **R-squared** of about `0.15`)? If you're thinking that this should therefore vice-versa intuitively mean **underfitting**, actually that's not right because **underfitting** is when the **generalizability** of a different model **linear form** specification that provides improved **model performance** is **validated**. What were seeing here, the variable, is something else..._\n",
    ">        \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot) But if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3d9295",
   "metadata": {},
   "source": [
    "Code and Explanation for Model Performance Comparison:\n",
    "I created a loop to train and evaluate a linear regression model across multiple random data splits. For each iteration, I calculated both in-sample and out-of-sample \n",
    "ùëÖ\n",
    "2\n",
    "  scores. The purpose was to assess how well the model fits the training data and how it generalizes to unseen data.\n",
    "\n",
    "The code below iterates 100 times, splitting the dataset into training and test sets, then calculates and stores the \n",
    "ùëÖ\n",
    "2\n",
    "  values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15da6c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Example data (replace with your dataset)\n",
    "X = np.random.rand(100, 3)\n",
    "y = 3 * X[:, 0] + 2 * X[:, 1] - X[:, 2] + np.random.normal(0, 0.5, 100)\n",
    "\n",
    "# Store R^2 values\n",
    "in_sample_r2, out_of_sample_r2 = [], []\n",
    "\n",
    "# Loop for multiple random splits\n",
    "for i in range(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    model = LinearRegression().fit(X_train, y_train)\n",
    "    \n",
    "    # In-sample and out-of-sample R^2\n",
    "    in_sample_r2.append(r2_score(y_train, model.predict(X_train)))\n",
    "    out_of_sample_r2.append(r2_score(y_test, model.predict(X_test)))\n",
    "\n",
    "# Visualization\n",
    "plt.plot(in_sample_r2, label='In-sample $R^2$', marker='o')\n",
    "plt.plot(out_of_sample_r2, label='Out-of-sample $R^2$', marker='x')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('$R^2$ Score')\n",
    "plt.title('In-sample vs. Out-of-sample $R^2$ Scores')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c32a675",
   "metadata": {},
   "source": [
    "Purpose and Meaning of Results:\n",
    "In-sample \n",
    "ùëÖ\n",
    "2\n",
    "R \n",
    "2\n",
    "  shows the model's fit on training data, while out-of-sample \n",
    "ùëÖ\n",
    "2\n",
    "R \n",
    "2\n",
    "  indicates generalization performance on unseen test data. A small gap between the two suggests good generalization, whereas a large gap may indicate overfitting.\n",
    "Avoiding np.random.seed() inside the loop ensures each iteration uses a different random data split, providing a more robust evaluation of model stability and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0145e1",
   "metadata": {},
   "source": [
    "### 9. Work with a ChatBot to understand the meaning of the illustration below; and, explain this in your own words<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _While we had seemed to **validate** the **generalizability** of `model7_fit` in **model building** exercise of the previous \"Question 7\" above, as well as the improved **model performance** of `model7_fit` comapred to `model6_fit`, the `model7_fit` model was always nonetheless more complex than `model6_fit` model (as seen by comparing their `.summary()` methods). This complexity, despite the minimal concerns regarding **multicollinearity**, should always have suggested some room for caution. This is because, as previously discussed in \"Question 6\" above, a complex **linear form** specification can allow a \"**model fit** to 'detect' idiosyncratic associations spuriously present specifically in the **training** dataset but which did not **generalize** to the **testing** dataset.\" Indeed, a close look at the **p-values** in `model7_fit.summary()` will show that the **evidence** (in the data) for many of the **estimated coefficients** of `model7_fit` is in fact not very strong. In comparision, the **evidence** (in the data) for many of the **estimated coefficients** of `model6_fit.summary()` is consistently stronger._\n",
    ">\n",
    "> _As discussed towards the end of the commentary in the previous \"Question 7\" above, the primary purpose of **multiple linear regression** methodology is to allow us to assess the **evidence** (in the data) for a given **linear form** specification based on **coefficient hypothesis testing**. In this regard, then, `model6_fit` might be preferred over `model7_fit` despite the better \"out of sample\" **model performance** of `model7_fit` over `model6_fit`. This may not be enough to convince everyone however, so an additional consideration that might be made here is that the more simpler (more parsimoneous) nature of `model6_fit` should be preferred over `model7_fit` from the perspective of **model interpretability**. Indeed, it is quite unclear how exactly one should think about and understand a four-way **interaction** variable such as `Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")` in conjunction with the whole host of the additional lower order interations. From a **model interpretability** perspective, understanding the meaning of the complex specification of `model7_fit` is \"challenging\" and \"complicated\" to say the least._\n",
    ">\n",
    "> - _There are also often circumstances where **model interpretability** can be MORE IMPORTANT than raw **model performance** in \"out of sample\" **prediction**._\n",
    "> - _This is ESPECIALLY true if **predictive model performance** is relatively comparable between models of two different complexity levels. In such cases, the benefits of better **model interpretability** might provide a clear argument for the simpler (more parsimoneous) model, not to mention the additional potential benefit of more consistent improved **generalizability** over the the more complex model this might offer._\n",
    ">\n",
    "> _This question drives home the point that a simpler (more parsimoneous) model always offers the potential benefit of more consistent **generalizability**, not to mention **interpretability**, over more complex models. We should *ONLY* use increasingly complex models that without questions outperfrm simler models. The code below illustrates this by further additionally raising the consideration that the random **train-test** approach used above is actually not the most natural one available for our dataset, which has different \"Generations\". In fact, if we were actually using this model to make **predictions**, we would increasingly acquire more data over time which we would use to make **precictions** about future data which we haven't yet seen, which is what the code demonstrates. And low and behold, this exposes **generalizability** concerns that we missed when we used the dataset in an idealized way and not actually how we would use such a dataset in practice in the real world (where data would arrive sequentially, and current data is used to predict future data). These **generalizability** concerns do affect both models, but the appear to be more problematic for `model7_fit` than `model6_fit`, which is certainly a result of the increased complexity of `model7_fit` which always opens up the possibility of model **overfitting**._\n",
    "\n",
    "<details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d57caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model7_gen1_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model7_gen1_predict_future_fit = model7_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model7_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628daf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model7_gen1to5_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model7_gen1to5_predict_future_fit = model7_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model7_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c52c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "model6_gen1_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model6_gen1_predict_future_fit = model6_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model6_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea14cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model6_gen1to5_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model6_gen1to5_predict_future_fit = model6_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model6_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2a3810",
   "metadata": {},
   "source": [
    "Trade-off Between Model Complexity and Generalizability\n",
    "Model Complexity:\n",
    "\n",
    "Model7_fit is more complex, incorporating additional predictors and interactions, which can improve in-sample performance. However, this complexity risks overfitting, where the model captures noise rather than true relationships.\n",
    "Model6_fit is simpler, with fewer predictors and interactions, making it less likely to overfit and more stable across different datasets.\n",
    "Generalizability:\n",
    "\n",
    "Generalizability refers to how well a model performs on unseen data. Despite model7_fit potentially showing better performance in-sample and out-of-sample, its complexity may limit its ability to generalize consistently.\n",
    "A simpler model like model6_fit, while slightly less accurate, may generalize better and avoid overfitting, leading to more reliable predictions on future data.\n",
    "Why Prefer a Simpler Model?:\n",
    "\n",
    "Simpler models are easier to interpret and less prone to overfitting. They focus on the most important relationships, ensuring better long-term predictability and consistency.\n",
    "Overly complex models, like model7_fit, may show high performance in the current dataset but fail to generalize well to new, unseen data.\n",
    "Sequential Data Considerations:\n",
    "\n",
    "Using sequential data for training and testing should respect the temporal order of observations to avoid data leakage and overoptimistic results. For example, training on past data to predict future data is more realistic than random train-test splits.\n",
    "Sequential data often requires additional feature engineering (e.g., lagged variables) to capture time-based patterns, which may further complicate the model but could improve prediction accuracy.\n",
    "Conclusion:\n",
    "While model7_fit offers better performance metrics, its increased complexity introduces risks of overfitting and reduces interpretability. A simpler model like model6_fit may be preferred for better generalizability and interpretability, especially if the performance difference between models is minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992924af",
   "metadata": {},
   "source": [
    "## Recommended Additional Useful Activities [Optional]\n",
    "\n",
    "The \"Ethical Profesionalism Considerations\" and \"Current Course Project Capability Level\" sections below **are not a part of the required homework assignment**; rather, they are regular weekly guides covering (a) relevant considerations regarding professional and ethical conduct, and (b) the analysis steps for the STA130 course project that are feasible at the current stage of the course\n",
    "\n",
    "<br>\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Ethical Professionalism Considerations</u></summary>\n",
    "\n",
    "### Ethical Professionalism Considerations\n",
    "\n",
    "This week addresses **multiple linear regression**, perhaps best exemplified through the consideration on **interactions** and their impact on the **model interpretation**, **evidence** and **validity* of models using **coefficient hypothesis testing** and \"in sample\" versus \"out of sample\" **model performance** comparision. Exactly, as in **simple linear regression**, the correctness of **p-values** used to give **evidence** for **predictive associations** that are **estimated** from a dataset depends on the (at least approximate) \"truth\" of the assumptions of the **multiple linear regression**, which are the same as those of the **simple linear regression** with the exception that the specification **linear form** can now model a much richer set of relationships between **predictor** and **outcome variables** based on **predictive associations** observed and **evidenced** in the data. With all this in mind, and reflecting back on the **Ethical Professionalism Considerations** from the previous week concerning **simple linear regression**...\n",
    "\n",
    "> - Which of the methods used for diagnostically assessing the assumptions of a **simple linear regression** specification could be used analogously generalized to the **multiple linear regression** context for the same purpose? \n",
    "> \n",
    "> - Examining the assumption of the **linear form** is more challenging in **multiple linear context**, but can be done using so-called **partial regression** (or **added variable**) **plot**. Is a ChatBot able to provide code to perform this diagnostic and instructions regarding its purpose, interpretation, and appropriate usage?\n",
    ">     \n",
    "> - Are there other diagnostic analyses that a ChatBot might suggest for you to help you evaluate the appropriateness of the assumptions of **fitted multiple linear regression model** you are considering using for **interpretation** or **prediction**? And if so, s the ChatBot able to provide code to perform these additional diagnostic and instructions regarding their purpose, interpretation, and appropriate usages?\n",
    ">     \n",
    "> - What do you think your ethical and professional responsibilites are when it comes to using and leveraging **multiple linear regression** methodology (and associated assumptions therein) in your work? To illustrate and demonstrate your thoughts on these considerations, can you give any specific examples of decisions that might be made during your process of executing a **multiple linear regression** that could have ethical and professional implications, risks, or consequences? What do you think are the simplest steps can you take to ensure that the conclusions of your work are both valid and reliable? What steps do you think are the most challenging from a practical perspective? \n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Current Course Project Capability Level</u></summary>\n",
    "\n",
    "**Remember to abide by the [data use agreement](https://static1.squarespace.com/static/60283c2e174c122f8ebe0f39/t/6239c284d610f76fed5a2e69/1647952517436/Data+Use+Agreement+for+the+Canadian+Social+Connection+Survey.pdf) at all times.**\n",
    "\n",
    "Information about the course project is available on the course github repo [here](https://github.com/pointOfive/stat130chat130/tree/main/CP), including a draft [course project specfication](https://github.com/pointOfive/stat130chat130/blob/main/CP/STA130F23_course_project_specification.ipynb) (subject to change). \n",
    "- The Week 01 HW introduced [STA130F24_CourseProject.ipynb](https://github.com/pointOfive/stat130chat130/blob/main/CP/STA130F24_CourseProject.ipynb), and the [available variables](https://drive.google.com/file/d/1ISVymGn-WR1lcRs4psIym2N3or5onNBi/view). \n",
    "- Please do not download the [data](https://drive.google.com/file/d/1mbUQlMTrNYA7Ly5eImVRBn16Ehy9Lggo/view) accessible at the bottom of the [CSCS](https://casch.org/cscs) webpage (or the course github repo) multiple times.\n",
    "    \n",
    "> ### NEW DEVELOPMENT<br>New Abilities Achieved and New Levels Unlocked!!!    \n",
    "> \n",
    "> \"Question 3\" as described below only addresses **multiple linear regression**... but you'll also perhaps notice that \"Question 3\" as described above extends this to address addresses **logistic regression**.\n",
    "    \n",
    "### Current Course Project Capability Level \n",
    "    \n",
    "This homework's \"Question 3\" introduced the idea of performing some **multiple linear regression** analyses on dataset from the Canadian Social Connection Survey. While other questions of this homework focussed on other datasets, the general analyses and principles they introduce are no doubt informative and applicable to this the dataset for our course project. Ideally, this should put you in a position to quite proficiently perform **multiple linear regression** analyses for the course project if you so desire and find appropriate for the objectives of you course project submission. Thus, the following (and more) should be possible at this stage... \n",
    "\n",
    "1. Select multiple **predictors predictor** from the Canadian Social Connection Survey data and examine how they jointly influence an outcome variable, paying special attention to the inclusion and interpretation of **categorical** and **indicator variables** and **interactions** (in terms of \"baseline\" reference groups and \"contrast\" or \"offsets\").\n",
    "\n",
    "2. Visualize different kinds of **predictive association** relationships, including **interactions** and relationship between **predictor** and the **outcome** variables that change across different levels of other **categorical** or **indicator predictor variables**, using tools like `plotly.express`.\n",
    "\n",
    "3. Use **coefficient hypothesis testing** and \"in sample\" versus \"out of sample\" **model performance** evaluation to perform **model building** and examine **generalizability** of **fitted models**.\n",
    "       \n",
    "4. Assess the presence of **multicollinearity** by considering the **condition numbers** of **fitted models** (with \"centering and scaling\") and their subsequent potential implications on **generalizability** of **fitted models**; and, perhaps even examine **pairwise correlation** and/or **variance inflation factors** for each **predictor variable** if you're feeling extra ambitious and want to go well \"above and beyond\" (in which case you could also consider the relationship between **multicollinearity** and why one level of a **categorical** variable is always omitted).\n",
    "\n",
    "5. Compare and contrast such analyses and their benefits with previous methodologies introduced and considered in the course.\n",
    "    \n",
    "6. Explore using model diagnostic to check assess the assumptions of your **multiple linear regression** analyses, and reflect on how failurse of these assumptions might impact the reliability of your findings and conlusions derived from your **fitted model**.\n",
    "\n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
